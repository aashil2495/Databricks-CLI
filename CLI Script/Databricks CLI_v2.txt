Login to Azure Portal.
az login

Create Resource Group named ashil-resourcegroup, you can replace it with your own name.
az group create --name ashil-resourcegroup --location southcentralus

Create a premium Databricks workspace named "myDatabricksWorkspace" in "ashil-resourcegroup" resource group at location "southcentralus".
az databricks workspace create --resource-group ashil-resourcegroup --name ashilDatabricksWorkspace --location southcentralus --sku premium

Create Storage account named "ashilstorage"
az storage account create --name ashilstorage --resource-group ashil-resourcegroup --location southcentralus --sku Standard_LRS --kind StorageV2 --hns true

Create Container named "ashilstorage" in storage account
az storage container create -n metastore-container --account-name ashilstorage --auth-mode login

Create Storage account named "ashilstoragedatastore"
az storage account create --name ashilstoragedatastore --resource-group ashil-resourcegroup --location southcentralus --sku Standard_LRS --kind StorageV2 --hns true

Create Container named "ashilstoragedatastore" in storage account
az storage container create -n raw --account-name ashilstoragedatastore --auth-mode login

Create Container named "ashilstoragedatastore" in storage account
az storage container create -n bronze --account-name ashilstoragedatastore --auth-mode login

Create Container named "ashilstoragedatastore" in storage account
az storage container create -n silver --account-name ashilstoragedatastore --auth-mode login

Create Container named "ashilstoragedatastore" in storage account
az storage container create -n gold --account-name ashilstoragedatastore --auth-mode login
------------------------
List the containers in storage account.
az storage container list --account-name ashilstorage --auth-mode login --query "[].name" -o table

Configure the Databricks PAT Token
databricks configure --token

Create unity catalog metastore in Databricks account
databricks unity-catalog metastores create --name southcentralus --storage-root abfss://metastore-container@ashilstorage.dfs.core.windows.net/ --region southcentralus

Assign metastore to workspace
databricks unity-catalog metastores assign --workspace-id 2744189980357297 --metastore-id d590c7f3-5713-43f6-886e-019e9cce2aca --default-catalog-name main

Create Databricks access connector
az databricks access-connector create --resource-group ashil-resourcegroup --name ashil-access-connector --location southcentralus --identity-type SystemAssigned

List Datarbicks Access connector
az databricks access-connector show --resource-group ashil-resourcegroup --name ashil-access-connector --query "identity.principalId" -o tsv

List Azure subscriptions
az account list --output table

List storage accounts
az storage account list --output table

Assign Role to Managed Identity(Access Connector) on "ashilstorage" storage account
az role assignment create --assignee 82eb651a-a93c-4bb9-970b-c61f48d6e346 --role "Storage Blob Data Contributor" --scope "/subscriptions/d066615d-e286-4bf1-92e7-7df087f1f4d0/resourceGroups/ashil-resourcegroup/providers/Microsoft.Storage/storageAccounts/ashilstorage"

az role assignment create --assignee 82eb651a-a93c-4bb9-970b-c61f48d6e346 --role "Storage Blob Data Contributor" --scope "/subscriptions/d066615d-e286-4bf1-92e7-7df087f1f4d0/resourceGroups/ashil-resourcegroup/providers/Microsoft.Storage/storageAccounts/ashilstoragedatastore"

-----------------------------------------
List access connectors
az databricks access-connector list --output table

Create storage credentails named "common-storage-cred"
databricks unity-catalog storage-credentials create --name common-storage-cred --az-mi-access-connector-id "/subscriptions/d066615d-e286-4bf1-92e7-7df087f1f4d0/resourceGroups/ashil-resourcegroup/providers/Microsoft.Databricks/accessConnectors/ashil-access-connector"

Create External Location named "raw-loc" with storage credentails named "common-storage-cred"
databricks unity-catalog external-locations create --name raw-loc --url abfss://raw@ashilstoragedatastore.dfs.core.windows.net/ --storage-credential-name common-storage-cred

Create External Location named "bronze-loc" with storage credentails named "common-storage-cred"
databricks unity-catalog external-locations create --name bronze-loc --url abfss://bronze@ashilstoragedatastore.dfs.core.windows.net/ --storage-credential-name common-storage-cred
Create External Location named "silver-loc" with storage credentails named "common-storage-cred"
databricks unity-catalog external-locations create --name silver-loc --url abfss://silver@ashilstoragedatastore.dfs.core.windows.net/ --storage-credential-name common-storage-cred
Create External Location named "gold-loc" with storage credentails named "common-storage-cred"
databricks unity-catalog external-locations create --name gold-loc --url abfss://gold@ashilstoragedatastore.dfs.core.windows.net/ --storage-credential-name common-storage-cred
-----------------------------------

Create schema called "bronze" under "main" catalog
databricks unity-catalog schemas create --catalog-name main --name bronze

Create schema called "silver" under "main" catalog
databricks unity-catalog schemas create --catalog-name main --name silver

Create schema called "gold" under "main" catalog
databricks unity-catalog schemas create --catalog-name main --name gold

Login with azcopy
azcopy login

Copy files from local path to raw container on ashilstoragedatastore
azcopy copy "C:\Users\aashi\projects\synthea\synthea\output\csv\2024_12_21T17_44_38Z\patients.csv" "https://ashilstoragedatastore.blob.core.windows.net/raw/patients.csv"

azcopy copy "C:\Users\aashi\projects\yellow_tripdata_2024-02.parquet" "https://ashilstoragedatastore.blob.core.windows.net/raw/"

-------------------------

Create job with file containing the job config.Replace your file path.
databricks jobs create --json-file C:\Users\aashi\projects\Databricks_Job_Config.json

Run job.Replace the job id.
databricks jobs run-now --job-id 189025494907676

Create a CLuster
databricks clusters create --json-file C:\Users\aashi\projects\Databricks_Cluster_Config.json
